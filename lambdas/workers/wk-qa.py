# -*- coding: utf-8 -*-
import os
import json
import boto3
import psycopg2
import google.generativeai as genai
from typing import Any, Dict
from services.utils import trigger_run_tick
import logging

logger = logging.getLogger(__name__)
logger.setLevel(os.environ.get("LOG_LEVEL", "INFO").upper())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BUCKET = os.environ["BUCKET_NAME"]
GEMINI_KEY = os.environ["GEMINI_API_KEY"]
AWS_REGION = os.getenv("AWS_REGION", "ap-northeast-2")
L_RUN_TICK = os.getenv("L_RUN_TICK")

s3 = boto3.client("s3", region_name=AWS_REGION)
lambda_client = boto3.client("lambda", region_name=AWS_REGION)

# Gemini ì„¤ì •
genai.configure(api_key=GEMINI_KEY)
model = genai.GenerativeModel("gemini-2.5-flash")

# ì„ë² ë”© ì„¤ì •
EMBED_MODEL   = os.environ.get("EMBED_MODEL", "text-embedding-3-small")
OPENAI_SDK    = os.environ.get("OPENAI_SDK", "v1")
PGVECTOR_DIM  = int(os.environ.get("PGVECTOR_DIM", "1536"))
_openai_client = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DB ì—°ê²° ë° ìƒíƒœ ê°±ì‹  í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_connection():
    return psycopg2.connect(
        host=os.environ["PGHOST"],
        user=os.environ["PGUSER"],
        password=os.environ["PGPASSWORD"],
        port=os.environ.get("PGPORT", "5432"),
        dbname=os.environ["PGDATABASE"],
        sslmode="require",
        connect_timeout=5,
    )

def upsert_status(cur, worker: str, run_id: int, payload: dict):
    cur.execute(
        """
        INSERT INTO run_results (worker_type, run_id, payload)
        VALUES (%s, %s, %s::jsonb)
        ON CONFLICT (run_id, worker_type) DO UPDATE
        SET payload = EXCLUDED.payload
        """,
        (worker, run_id, json.dumps(payload, ensure_ascii=False)),
    )

def get_openai_client():
    global _openai_client
    if _openai_client is not None:
        return _openai_client

    api_key = os.environ["OPENAI_API_KEY"]
    if OPENAI_SDK == "v1":
        from openai import OpenAI
        _openai_client = ("v1", OpenAI(api_key=api_key))
    else:
        import openai as v0
        v0.api_key = api_key
        _openai_client = ("v0", v0)
    return _openai_client

def embed_question(text: str) -> list[float]:
    text = (text or "").strip()
    if not text:
        # ë¹„ì–´ ìˆìœ¼ë©´ ê·¸ëƒ¥ 0 ë²¡í„° ë¦¬í„´ (ì‹¤ì œë¡œëŠ” ê±°ì˜ ì•ˆíƒ€ê²Œ)
        return [0.0] * PGVECTOR_DIM

    sdk, client = get_openai_client()
    if sdk == "v1":
        r = client.embeddings.create(model=EMBED_MODEL, input=[text])
        emb = r.data[0].embedding
    else:
        r = client.Embedding.create(model=EMBED_MODEL, input=[text])
        emb = r["data"][0]["embedding"]

    if len(emb) != PGVECTOR_DIM:
        raise RuntimeError(f"QA embedding dim mismatch: {len(emb)} != {PGVECTOR_DIM}")
    return emb

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# documents_chunks ì—ì„œ ìƒìœ„ Kê°œ ì²­í¬ + ì•µì»¤ ì¡°íšŒ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_top_chunks_for_qa(cur, doc_id: int, q_vec: list[float], k: int = 8):
    """
    pgvector similarity (embedding <-> q_vec) ê¸°ë°˜ìœ¼ë¡œ
    ìƒìœ„ Kê°œ ê´€ë ¨ ì²­í¬ì™€ ê·¸ ì•µì»¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¨ë‹¤.
    """
    # q_vec ì„ ê·¸ëŒ€ë¡œ vector ë¡œ ìºìŠ¤íŒ…í•´ì„œ ì‚¬ìš© (doc-embedding ë•Œì™€ ë™ì¼ íŒ¨í„´)
    cur.execute(
        """
        SELECT chunk_idx, content, anchors
        FROM documents_chunks
        WHERE doc_id = %s
        ORDER BY embedding <-> %s::vector
        LIMIT %s;
        """,
        (doc_id, q_vec, k),
    )
    rows = cur.fetchall()
    out = []
    for idx, content, anchors in rows:
        # anchors: {"count":N,"items":[...]} í˜•íƒœ
        if isinstance(anchors, str):
            try:
                anchors = json.loads(anchors)
            except Exception:
                anchors = {}
        if anchors is None:
            anchors = {}
        out.append(
            {
                "chunk_idx": idx,
                "content": content or "",
                "anchors": anchors or {"count": 0, "items": []},
            }
        )
    return out

def build_context_from_chunks(chunks_for_qa: list[dict], max_chars_per_chunk: int = 1200) -> str:
    """
    LLM ì…ë ¥ìš© ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ êµ¬ì„±:
    - ì²­í¬ë³„ index, anchor ì œëª©ë“¤, ë‚´ìš© ì¼ë¶€ë¥¼ í¬í•¨
    - ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡ ì²­í¬ë‹¹ ìµœëŒ€ ê¸€ììˆ˜ ì œí•œ
    """
    blocks: list[str] = []
    for ch in chunks_for_qa:
        idx = ch["chunk_idx"]
        text = ch["content"] or ""
        anchors = ch.get("anchors") or {}
        items = anchors.get("items") or []

        titles = []
        for a in items:
            if isinstance(a, dict):
                t = (a.get("title") or "").strip()
                if t:
                    titles.append(t)
        titles_str = ", ".join(titles) if titles else "(ê´€ë ¨ ì•µì»¤ ì—†ìŒ)"

        snippet = text[:max_chars_per_chunk]
        blocks.append(
            f"[chunk {idx}] anchors: {titles_str}\n{snippet}"
        )

    return "\n\n".join(blocks) if blocks else "(ê´€ë ¨ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í•¨)"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì§ˆë¬¸ íŒŒì‹± í—¬í¼ (ìˆ˜ì • ìš”ì²­ ë¶€ë¶„ ì œê±°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_pure_question(original_question: str) -> str:
    """
    question ì•ˆì—ì„œ 'ì˜¤íƒˆì/í‘œí˜„ ìˆ˜ì •/ëŒ€ì²´ í‘œí˜„/ë¬¸êµ¬ ë‹¤ë“¬ê¸°' ê°™ì€
    'ë¬¸ì¥ ìˆ˜ì •Â·ëŒ€ì²´ ìš”ì²­' ë¶€ë¶„ì€ ëª¨ë‘ ë¬´ì‹œí•˜ê³ ,
    ì‹¤ì œë¡œ ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ê¶ê¸ˆí•´í•˜ëŠ” ì§ˆì˜ ë¶€ë¶„ë§Œ ë½‘ëŠ”ë‹¤.

    - ì‹¤íŒ¨í•˜ê±°ë‚˜ ë¹„ì–´ ìˆìœ¼ë©´ ì›ë˜ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜.
    """
    if not original_question:
        return ""

    # revision ì„±í–¥ í‚¤ì›Œë“œê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ íŒŒì‹± ì‹œë„
    revision_like_keywords = [
        "ì˜¤íƒˆì", "ì˜¤íƒ€",
        "ìˆ˜ì •í•´ì¤˜", "ìˆ˜ì •í•´ ì£¼ê³ ", "ìˆ˜ì •í•´ì£¼ê³ ", "ìˆ˜ì •í•´", "ìˆ˜ì •í•´ë¼", "ìˆ˜ì •",
        "ê³ ì³ì¤˜", "ê³ ì³ ì£¼ê³ ", "ê³ ì³ì£¼ê³ ", "ê³ ì³", "ê³ ì¹˜", "ê³ ì³ë¼",
        "ë°”ê¿”ì¤˜", "ë°”ê¿” ì£¼ê³ ", "ë°”ê¿”ì£¼ê³ ", "ë°”ê¿”", "ë°”ê¾¸", "ë³€ê²½í•´ì¤˜", "ë³€ê²½í•´",
        "ëŒ€ì²´", "ëŒ€ì²´í•´ì¤˜", "ëŒ€ì²´í•´", "ëŒ€ì²´ í‘œí˜„", "ëŒ€ì²´ ë¬¸êµ¬",
        "í‘œí˜„ ë‹¤ë“¬", "ë¬¸êµ¬ ë‹¤ë“¬", "ë‹¤ë“¬ì–´", "ë‹¤ë“¬ì–´ì¤˜",
        "ë¬¸êµ¬ ì œì•ˆ", "ë¬¸êµ¬ ì¶”ì²œ", "í…œí”Œë¦¿", "ë‹¤ë¥¸ í‘œí˜„", "ë‹¤ë¥¸ ë¬¸êµ¬",
    ]

    if not any(k in original_question for k in revision_like_keywords):
        # ìˆ˜ì • ê´€ë ¨ í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        return original_question

    splitter_prompt = f"""
ë‹¹ì‹ ì€ í•œêµ­ì–´ ê³„ì•½ì„œ QA ì‹œìŠ¤í…œì˜ ì „ì²˜ë¦¬ ëª¨ë“ˆì´ë‹¤.

ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ:
- 'ì˜¤íƒˆì ìˆ˜ì •í•´ì¤˜', 'ë¬¸êµ¬ë¥¼ ë‹¤ë“¬ì–´ì¤˜', 'ëŒ€ì²´ í‘œí˜„ì„ ì œì•ˆí•´ì¤˜', 'ìˆ˜ì •í•´ì£¼ê³ ' ë“±
  **ìœ„í—˜ ì¡°í•­/ë¬¸ì¥/í‘œí˜„ì„ ìˆ˜ì •Â·ëŒ€ì²´í•´ ë‹¬ë¼ëŠ” ìš”ì²­ ë¶€ë¶„ì€ ëª¨ë‘ ì œê±°**í•˜ê³ ,
- ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ **ì •ë³´ë¥¼ ë¬»ëŠ” ë¶€ë¶„(ì§ˆë¬¸)**ë§Œ ìì—°ìŠ¤ëŸ½ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹¤ì‹œ ì¨ë¼.

ì˜ˆì‹œ:
- ì…ë ¥: "ì˜¤íƒˆì ê³ ì³ì£¼ê³ , ê³„ì•½ ì¢…ë£Œ ì‹œ ê¸°ë°€ì •ë³´ëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ”ê°€?"
  ì¶œë ¥: "ê³„ì•½ ì¢…ë£Œ ì‹œ ê¸°ë°€ì •ë³´ëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ”ê°€?"

- ì…ë ¥: "ìœ„í—˜ ì¡°í•­ ìˆ˜ì •í•´ì£¼ê³ , ìœ„ì•½ê¸ˆì€ ì–´ëŠ ì •ë„ê¹Œì§€ ì²­êµ¬í•  ìˆ˜ ìˆëŠ”ì§€ ì•Œë ¤ì¤˜."
  ì¶œë ¥: "ìœ„ì•½ê¸ˆì€ ì–´ëŠ ì •ë„ê¹Œì§€ ì²­êµ¬í•  ìˆ˜ ìˆëŠ”ì§€ ì•Œë ¤ì¤˜."

- ì…ë ¥: "ì´ ì¡°í•­ì„ ë” ë¶€ë“œëŸ¬ìš´ í‘œí˜„ìœ¼ë¡œ ë°”ê¿”ì¤˜."
  ì¶œë ¥: ""   (ì •ë³´ ì§ˆì˜ê°€ ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ë¬¸ìì—´)

ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ:
{{ "pure_question": "<ì •ë³´ ì§ˆì˜ë§Œ ë‚¨ê¸´ ì§ˆë¬¸(ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)>" }}

[ì›ë³¸ ì§ˆë¬¸]
{original_question}
""".strip()

    try:
        res = model.generate_content(
            splitter_prompt,
            generation_config=genai.types.GenerationConfig(
                response_mime_type="application/json"
            ),
        )
        data = json.loads(res.text)
        pure_q = (data.get("pure_question") or "").strip()
        if pure_q:
            return pure_q
        # ë¹„ì–´ ìˆìœ¼ë©´ ì›ë˜ ì§ˆë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        return original_question
    except Exception:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ ì•ˆì „í•˜ê²Œ ì›ë˜ ì§ˆë¬¸ ì‚¬ìš©
        return original_question

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì´ë²¤íŠ¸ íŒŒì‹±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_event(event: Dict[str, Any]) -> Dict[str, Any]:
    body = None
    if isinstance(event, dict) and "body" in event:
        raw = event.get("body")
        if isinstance(raw, str) and raw.strip():
            try:
                body = json.loads(raw)
            except Exception:
                body = {}
        elif isinstance(raw, dict):
            body = raw
        else:
            body = {}
    else:
        body = event if isinstance(event, dict) else {}

    run_id = body.get("runId") or body.get("run_id")
    doc_id = body.get("docId") or body.get("doc_id")
    session_id = body.get("sessionId") or body.get("session_id")
    inputs = body.get("inputs", {}) or {}

    if not all([run_id, doc_id, session_id]):
        raise ValueError("Missing required parameters: runId, docId, sessionId")

    return {
        "run_id": int(run_id),
        "doc_id": int(doc_id),
        "session_id": int(session_id),
        "inputs": inputs,
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Lambda Handler
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def lambda_handler(event, context):
    conn = None
    cur = None
    try:
        params = parse_event(event)
        run_id = params["run_id"]
        doc_id = params["doc_id"]
        session_id = params["session_id"]
        inputs = params["inputs"]

        conn = get_connection()
        cur = conn.cursor()

        upsert_status(cur, "qa", run_id, {"status": "running"})
        conn.commit()

        # 1ï¸âƒ£ ë¬¸ì„œ ì´ë¦„
        cur.execute("SELECT name FROM documents WHERE id=%s;", (doc_id,))
        row = cur.fetchone()
        if not row:
            upsert_status(cur, "qa", run_id, {"status": "failed", "error": f"Document not found: {doc_id}"})
            conn.commit()
            return {"ok": False, "error": f"Document not found: {doc_id}"}
        doc_name = row[0]

        s3_key = (inputs or {}).get("s3TextKey") or f"output/{doc_name}.txt"

        # 2ï¸âƒ£ ì„¸ì…˜ ì •ë³´
        cur.execute("SELECT role, answers FROM sessions WHERE id=%s;", (session_id,))
        fetched = cur.fetchone()
        if not fetched:
            upsert_status(cur, "qa", run_id, {"status": "failed", "error": f"Session not found: {session_id}"})
            conn.commit()
            return {"ok": False, "error": f"Session not found: {session_id}"}

        role_db, answers = fetched
        if isinstance(answers, str):
            try:
                answers = json.loads(answers)
            except Exception:
                answers = {}

        role = (inputs.get("role") if inputs else None) or role_db or answers.get("role")
        question_raw = (inputs.get("question") if inputs else None) or answers.get("question")
        focus = (inputs.get("focus") if inputs else None) or answers.get("focus", [])
        if isinstance(focus, str):
            focus = [focus]
        focus_joined = ", ".join(map(str, focus)) if focus else "ì—†ìŒ"

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # â‘  question ì´ ìˆìœ¼ë©´: ìˆ˜ì •ìš”ì²­ ë¶€ë¶„ ì œê±° + ìˆœìˆ˜ ì§ˆì˜ë§Œ ì‚¬ìš©
        # â‘¡ question ì´ ì—†ê³  focus ë§Œ ìˆìœ¼ë©´: focus ê¸°ë°˜ìœ¼ë¡œ "ê°€ì§œ ì§ˆë¬¸" ìƒì„±
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if question_raw and question_raw.strip():
            # ì§ˆë¬¸ì´ ìˆëŠ” ê²½ìš°: revision ê´€ë ¨ í‘œí˜„ ì œê±° í›„ ìˆœìˆ˜ ì§ˆì˜ë§Œ ì‚¬ìš©
            pure_question = extract_pure_question(question_raw)
            question_for_qa = pure_question.strip() or question_raw.strip()
        elif focus:
            # ì§ˆë¬¸ì€ ì—†ê³  focus ë§Œ ìˆëŠ” ê²½ìš°:
            # â†’ focus ë¥¼ ì´ìš©í•´ "ë¬¸ì„œì—ì„œ ì´ í‚¤ì›Œë“œë“¤ì— ëŒ€í•´ ì¤‘ìš”í•œ ë‚´ìš©ì„ ì„¤ëª…í•´ ë‹¬ë¼"ëŠ” í˜•íƒœì˜ ì§ˆì˜ ìƒì„±
            question_for_qa = f"ë‹¤ìŒ í‚¤ì›Œë“œì— ëŒ€í•´ ì´ ê³„ì•½ì„œì—ì„œ ì‚¬ìš©ìì—ê²Œ ì¤‘ìš”í•œ ë‚´ìš©ê³¼ ìœ„í—˜ ìš”ì†Œë¥¼ ì„¤ëª…í•˜ë¼: {', '.join(map(str, focus))}"
        else:
            # ì´ ê²½ìš°ëŠ” ì‚¬ì‹¤ìƒ plan ë‹¨ê³„ì—ì„œ QAê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•„ì•¼ í•˜ì§€ë§Œ,
            # í˜¹ì‹œ ëª°ë¼ì„œ ì™„ì „ ë¹ˆ ë¬¸ìì—´ë¡œ ë‘ê¸°
            question_for_qa = ""

        # Verifier ì¬ì‹œë„ í”¼ë“œë°±
        retry_info = (inputs or {}).get("retryInfo") or {}
        vf_reason = retry_info.get("reason")
        vf_metrics = retry_info.get("metrics") or {}
        vf_attempt = retry_info.get("attempt")

        verifier_block = ""
        if vf_reason or vf_metrics:
            verifier_block = f"""
[ì´ì „ ì‹œë„ì— ëŒ€í•œ Verifier í”¼ë“œë°±]
- attempt: {vf_attempt}
- reason: {vf_reason or "N/A"}
- metrics: anchorRate={vf_metrics.get("anchorRate")}, kpri={vf_metrics.get("kpri")}, faithfulness={vf_metrics.get("faithfulness")}

ìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬, ë³´ë‹¤ ì •í™•í•˜ê³  ê·¼ê±°ê°€ ë¶„ëª…í•œ ë‹µë³€ì„ ë‹¤ì‹œ ì‘ì„±í•˜ë¼.
ê°€ëŠ¥í•˜ë©´ ì•µì»¤(ê´€ë ¨ ì¡°í•­ ì œëª©/ë²ˆí˜¸)ë¥¼ ë” ëª…í™•íˆ ì œì‹œí•˜ë¼.
""".strip()

        # ì¬ë¶„ì„ ì»¨í…ìŠ¤íŠ¸
        reanalyze_text = (inputs or {}).get("prevSummaryText")
        reanalyze_block = ""
        if reanalyze_text:
            reanalyze_block = f"""
[ì´ì „ ì‚¬ìš©ì ë¶„ì„ ìš”ì•½]
ì•„ë˜ëŠ” ì§ì „ ë¶„ì„(run)ì˜ í•µì‹¬ ìš”ì•½ì´ë‹¤. ì´ ë‚´ìš©ì„ ì°¸ê³ í•˜ë˜,
ì´ì „ ë‹µë³€ì—ì„œ ì¶©ë¶„íˆ ë‹¤ë£¨ì§€ ëª»í–ˆë˜ ë¶€ë¶„ì´ ì—†ëŠ”ì§€ ë‹¤ì‹œ ì ê²€í•˜ë¼.

{reanalyze_text}
""".strip()

        # 3ï¸âƒ£ ê´€ë ¨ ì²­í¬ + ì•µì»¤ ì¡°íšŒ (pgvector RAG)
        #    - ë¬¸ì„œ ì „ì²´ë¥¼ S3 ì—ì„œ ë‹¤ ì½ì–´ì˜¤ëŠ” ëŒ€ì‹ ,
        #      question_for_qa ì„ë² ë”© â†’ documents_chunks ìƒìœ„ Kê°œë§Œ LLMì— ì œê³µ
        q_vec = embed_question(question_for_qa)
        top_chunks = get_top_chunks_for_qa(cur, doc_id, q_vec, k=24)  # k 8 â†’ 24ë¡œ ëŠ˜ë¦¬ê¸°
        context_text = build_context_from_chunks(top_chunks, max_chars_per_chunk=800)  # ì²­í¬ë‹¹ 800ìë¡œ ì¤„ì—¬ì„œ ì „ì²´ ê¸¸ì´ ì¡°ì ˆ

        logger.info("[QA] doc_id=%s question=%r", doc_id, question_for_qa)
        logger.info("[QA] retrieved chunks = %d", len(top_chunks))
        for ch in top_chunks:
            anchors = ch.get("anchors") or {}
            items = anchors.get("items") or []
            titles = [ (a.get("title") or "").strip() for a in items if isinstance(a, dict) ]
            logger.info(
                "[QA] chunk_idx=%s anchors=%s snippet=%r",
                ch["chunk_idx"],
                titles,
                (ch["content"] or "")[:200].replace("\n", " ")
            )

        # 3ï¸âƒ£ S3 ë¬¸ì„œ ë¡œë“œ
        # obj = s3.get_object(Bucket=BUCKET, Key=s3_key)
        # text = obj["Body"].read().decode("utf-8")

        # 4ï¸âƒ£ Gemini í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì§ˆë¬¸ì€ question_for_qa ì‚¬ìš©)
        prompt = f"""
ë‹¹ì‹ ì€ ê³„ì•½ì„œ ë¶„ì„ ì „ë¬¸ê°€ì´ì QA Agentì…ë‹ˆë‹¤.
ì•„ë˜ 'ê´€ë ¨ ì²­í¬ë“¤'ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì§ì ‘ ë¶„ì„í•˜ê³ ,
ê°€ëŠ¥í•œ í•œ ì‹¤ì œ ì¡°í•­ ì œëª©/ë²ˆí˜¸(ì•µì»¤)ë¥¼ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”.

[ì‘ë‹µ ê·œì¹™ ë§¤ìš° ì¤‘ìš”]
1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë”ë¼ë„ ë˜ë¬»ê±°ë‚˜ ì¶”ê°€ ì§ˆë¬¸ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.
2. ì§ˆë¬¸ì´ í•œ ë¬¸ì¥ì— ì—¬ëŸ¬ ìš”êµ¬ë¥¼ ì„ì–´ ë§í•˜ë”ë¼ë„,
   ê·¸ ì¤‘ì—ì„œ "ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•œ ì •ë³´ ì§ˆì˜" ë¶€ë¶„ë§Œì„ ëŒ€ìƒìœ¼ë¡œ ë‹µë³€í•œë‹¤.
   ì˜ˆë¥¼ ë“¤ì–´, "ì˜¤íƒˆì ìˆ˜ì •í•´ì£¼ê³ , ê³„ì•½ ì¢…ë£Œ ì‹œ ê¸°ë°€ì •ë³´ëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ”ê°€?"
   ë¼ëŠ” ì§ˆë¬¸ì´ ì˜¤ë©´, "ê³„ì•½ ì¢…ë£Œ ì‹œ ê¸°ë°€ì •ë³´ëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ”ê°€?"ì— ëŒ€í•´ì„œë§Œ ë‹µë³€í•˜ë¼.
3. ë¬¸ì¥/í‘œí˜„ ìˆ˜ì •, ì˜¤íƒˆì/ë¬¸êµ¬ ë‹¤ë“¬ê¸°, ëŒ€ì²´ í‘œí˜„/í…œí”Œë¦¿ ì œì•ˆ ìš”ì²­ì€ ëª¨ë‘ ë¬´ì‹œí•˜ê³ ,
   ì‹¤ì œë¡œ ë¬¸ì„œì— ì–´ë–¤ ë‚´ìš©ì´ ê·œì •ë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•´ì„œë§Œ ë‹µë³€í•œë‹¤.
4. ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì‹ ë‹µë³€ ê¸ˆì§€. í•­ìƒ 'ë¶„ì„ ê²°ê³¼'ë§Œ JSONìœ¼ë¡œ ì¶œë ¥í•œë‹¤.
5. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ í—ˆêµ¬ë¡œ ìƒì„±í•˜ì§€ ì•ŠëŠ”ë‹¤.
6. ê´€ë ¨ ì¡°í•­ì´ ì—¬ëŸ¬ ê°œë©´ ëª¨ë‘ anchorsì— ë„£ëŠ”ë‹¤.
7. ì•„ë˜ [ê´€ë ¨ ì²­í¬ë“¤]ì€ ë¬¸ì„œ ì „ì²´ê°€ ì•„ë‹ˆë¼, ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì¼ë¶€ ì¡°ê°ë“¤ì´ë‹¤.
+    ë‹µë³€ì€ ì´ ì»¨í…ìŠ¤íŠ¸ ì•ˆì—ì„œë§Œ ê·¼ê±°ë¥¼ ì°¾ë˜,
+    ì»¨í…ìŠ¤íŠ¸ì— í•´ë‹¹ ë‚´ìš©ì´ ë³´ì´ì§€ ì•ŠëŠ” ê²½ìš°ì—ëŠ”
+    - "ì´ ì»¨í…ìŠ¤íŠ¸ ë²”ìœ„ ë‚´ì—ì„œëŠ” í•´ë‹¹ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ë‹¤"ê³  ë§í•˜ê³ ,
+    - ë¬¸ì„œ ì „ì²´ì— ì—†ë‹¤ê³  ë‹¨ì •í•˜ì§€ ë§ì•„ë¼.

[ì‚¬ìš©ì ì—­í• ]
{role}

[ì§ˆë¬¸ (ì •ë³´ ì§ˆì˜ë§Œ ì¶”ë ¤ì§„ ë²„ì „)]
{question_for_qa}

[ì§‘ì¤‘ í‚¤ì›Œë“œ]
{focus_joined}

{verifier_block if verifier_block else ""}

{reanalyze_block if reanalyze_block else ""}

[ê´€ë ¨ ì²­í¬ë“¤ (ìƒìœ„ ê²€ìƒ‰ ê²°ê³¼)]
{context_text}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
{{
  "answer": "ìš”ì•½ëœ ë‹µë³€ ë‚´ìš©",
  "anchors": ["ì œ15ì¡° (í•´ì§€ ë° í™˜ë¶ˆ)", "ì œ12ì¡° (ìš”ê¸ˆ ë°˜í™˜)"]
}}
""".strip()

        # 5ï¸âƒ£ Gemini API í˜¸ì¶œ
        result = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                response_mime_type="application/json"
            )
        )

        parsed = json.loads(result.text)

        print("[RESULT] doc_name =", doc_name)
        print("[RESULT] original_question =", question_raw)
        print("[RESULT] question_for_qa =", question_for_qa)
        print("[RESULT] gemini_answer =", parsed.get("answer"))
        print("[RESULT] gemini_anchors =", parsed.get("anchors"))

        # 6ï¸âƒ£ ê²°ê³¼ ì €ì¥
        payload = {
            "status": "done",
            "ok": True,
            "worker": "qa",
            "role": role,
            # ğŸ”§ ì‹¤ì œ QAì— ì‚¬ìš©í•œ ì§ˆë¬¸ê³¼ ì›ë³¸ ì§ˆë¬¸ ë‘˜ ë‹¤ ì €ì¥
            "question": question_for_qa,
            "originalQuestion": question_raw,
            "focus": focus,
            "answer": parsed.get("answer"),
            "anchors": parsed.get("anchors", []),
        }
        upsert_status(cur, "qa", run_id, payload)
        conn.commit()
        trigger_run_tick(run_id)

        return {"ok": True, "worker": "qa", "runId": run_id}

    except Exception as e:
        try:
            if conn:
                if not cur:
                    cur = conn.cursor()
                rid = None
                try:
                    rid = parse_event(event)["run_id"]
                except Exception:
                    pass
                if rid is not None:
                    upsert_status(cur, "qa", rid, {"status": "failed", "error": str(e)})
                    conn.commit()
                    trigger_run_tick(rid)
        except Exception:
            pass
        return {"ok": False, "error": str(e)}

    finally:
        if cur:
            cur.close()
        if conn:
            conn.close()
